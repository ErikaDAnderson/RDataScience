---
title: "22 Model"
date: "February 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load library
library(tidyverse) # basic packages
library(magrittr) # pipes
library(purrr) # functional programming ie map
library(modelr) # for models

# options for modelr to warn if there are NAs in data
# default is to silently drop them
options(na.action = na.warn)

```

# 23.2.1 A Simple Model Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

The slope pivots, and the intercept moves, due to outliers.

```{r}

# given to simulate data
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

# use a second simulate to compare
sim1b <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

# graph these simulated datasets
ggplot() +
  geom_point(aes(x,y), data = sim1a, color = "red") +
  geom_smooth(aes(x,y), data = sim1a, color = "red", method = lm) +
  geom_point(aes(x,y), data = sim1b, color = "blue") +
  geom_smooth(aes(x,y), data = sim1b, color = "blue", method = lm) +
  theme_bw()

# apply lm function
sim1a_mod <- lm(y ~ x, data = sim1a)
sim1b_mod <- lm(y ~ x, data = sim1b)

# display
coef(sim1a_mod)
coef(sim1b_mod)

# J Arnold used function with map to make multiple graphs from simulations to compare
# why not...

# make a function to make simulated data sets
simData <- function(i) {
    tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    id = i
  )
}

# apply that function nine times
sims <- map_df(1:9, simData)

# graph results
ggplot(sims, aes(x,y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~id) +
  theme_bw()
  
```

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}

# given
model_ea <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance2 <- function(mod, data) {
  diff <- data$y - model_ea(mod, data)
  mean(abs(diff))
}

measure_distance <- function(mod, data) {
  diff <- data$y - model_ea(mod, data)
  sqrt(mean(diff ^ 2))
}

```

Use optim() to fit this model to the simulated data above and compare it to the linear model.

```{r}

# read help file
?optim

# with mean difference
optimMean <- optim(c(0,0), measure_distance2, data = sim1b)
optimMean

# compare to root mean squared deviation
optimRMSD <- optim(c(0,0), measure_distance, data = sim1b)
optimRMSD


# graph them both
ggplot(aes(x,y), data = sim1) +
  geom_point() +
  #geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_abline(slope = optimMean$par[2], 
              intercept = optimMean$par[1], color = "red") +
  geom_abline(slope = optimRMSD$par[2], 
              intercept = optimRMSD$par[1], color = "blue") +
  theme_bw()

```

It looks like mean distance is similar to RMSD. It is possible that mean abs is les sensitive to outliers.

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

```{r}

model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}

```

It would only optimize in 2 dimensions, not the three necessary. J Arnold says that the origin of the optim function would give different results.

# 23.3.3 Visualizing Models Exercises

1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?

geom_smooth and loess model fit the data the same.

```{r}

# given
sim1_mod <- lm(y ~ x, data = sim1)

# model fitting with loess
sim1_mod_loess <- loess(y ~ x, data = sim1)

# generate grid & predictions
grid_loess <- sim1 %>%
  data_grid(x) %>%
  add_predictions(., sim1_mod_loess)

# graph
ggplot() +
  geom_point(aes(x,y), data = sim1) +
  geom_smooth(aes(x, y), method = "loess",  
              data = sim1, se = FALSE) +
  geom_line(aes(x, pred), data = grid_loess, 
            color = "red", se = FALSE) +
  theme_bw()
  

```


2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?

add_prediction adds a single new column, with default name pred, to the input data. 
spread_predictions adds one column for each model. 
gather_predictions adds two columns .model and .pred, and repeats the input rows for each model.

```{r}

# try out spread predictions
sim1 %>%
  data_grid(x) %>%
  spread_predictions(sim1_mod, sim1_mod_loess)

# try out gather predictions
sim1 %>%
  data_grid(x) %>%
  gather_predictions(sim1_mod, sim1_mod_loess)

```


3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

```{r}
?geom_ref_line
```

From the modelr package to add a reference line to the ggplot graph. It can be used to mark the zero mark for residuals to see if there are any patterns left in the data.

4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

J Arnold says that using the absolute value of the residual makes visualizing the spread easier by doubling the values. But this losses information about whether the model is systematically under or over estimating.

```{r}

# given
sim1_mod <- lm(y ~ x, data = sim1)

sim1 <- sim1 %>%
  add_residuals(sim1_mod) %>%
  
  # add column for absolute residuals
  mutate(absRes = abs(resid)) 

# graph both and compare
ggplot(sim1) +
  geom_freqpoly(aes(resid), binwidth = 0.5, color = "red") +
  geom_freqpoly(aes(absRes), binwidth = 0.5, color = "blue") +
  geom_ref_line(v = 0)


```


23.4.5 Formulas and Model Families Exercises

1. What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?

2. Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction?

3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

```

4. For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

